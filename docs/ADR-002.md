# ADR-002: CEF Framework Core Architecture - Domain Agnostic Design

**Status:** Accepted  
**Date:** 2024-11-24  
**Authors:** CEF Project Team  
**Supersedes:** ADR-001 (portions related to framework)

---

## Context

ADR-001 mixed framework concerns with example domain concerns. This ADR focuses purely on the framework architecture:
- **Domain agnostic** - no knowledge of "Patient", "Doctor", or any specific domain
- **Two simple interfaces** - Indexing and Retrieval only
- **JGraphT in-memory** - fast graph operations with dual persistence
- **MCP tool compatible** - Retrieval exposed as Model Context Protocol tool

The framework's primary purpose is to **enable reasoning through relationship paths** and provide **context keywords for Vector Similarity Search (VSS)**.

---

## Decision

### Core Principle

> **The framework provides graph reasoning paths and context keywords to enhance VSS retrieval. It does not impose domain schemas.**

---

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────┐
│                    User Application                          │
│              (Defines: Nodes, Relations, Chunks)             │
└─────────────────────────────────────────────────────────────┘
                             │
                 ┌───────────┴───────────┐
                 │   Framework Interfaces │
                 │                        │
                 │  1. KnowledgeIndexer   │
                 │  2. KnowledgeRetriever │
                 │     (MCP Tool)         │
                 └───────────┬────────────┘
                             │
┌─────────────────────────────────────────────────────────────┐
│              CEF Framework Core                              │
│                                                              │
│  ┌────────────────────┐      ┌─────────────────────────┐   │
│  │  Parser System     │      │  GraphStore (pluggable) │   │
│  │  • AbstractParser  │      │  • JGraphT (default)    │   │
│  │  • ParserFactory   │      │  • Neo4j (optional)     │   │
│  │  • ANTLR (complex) │◄─────┤  • TinkerPop (optional) │   │
│  └────────────────────┘      │  • Fast lookups/traverse│   │
│                              └─────────────────────────┘   │
│  ┌────────────────────┐      ┌─────────────────────────┐   │
│  │  DataSource        │      │  LLM Client Factory     │   │
│  │  • Filesystem      │      │  • OpenAI, Ollama       │   │
│  │  • BlobStorage     │      │  • vLLM, Mock           │   │
│  │  • S3/MinIO        │      │  • ContextProvider      │   │
│  └────────────────────┘      └─────────────────────────┘   │
│                              ┌─────────────────────────┐   │
│                              │    Context Assembler    │   │
│                              │  • Token Budgeting      │   │
│                              │  • Prioritization       │   │
│                              └─────────────────────────┘   │
│                                                              │
│  ┌────────────────────────────────────────────────────┐    │
│  │           Dual Persistence (Pluggable)              │    │
│  │  Every operation updates BOTH:                      │    │
│  │  • VectorStore (Postgres/DuckDB/Qdrant/Pinecone)  │    │
│  │  • GraphStore (JGraphT/Neo4j/TinkerPop)           │    │
│  └────────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────────┘
                             │
┌─────────────────────────────────────────────────────────────┐
│              Pluggable Storage Architecture                  │
│  ┌──────────────────────┐  ┌──────────────────────────┐   │
│  │   GraphStore API     │  │    VectorStore API       │   │
│  │  • Node              │  │  • Chunk                 │   │
│  │  • Edge              │  │  • Embedding (vector)    │   │
│  │  • RelationType      │  │  • BM25 Index            │   │
│  └──────────────────────┘  └──────────────────────────┘   │
│  Impl: JGraphT (default)    Impl: Postgres/DuckDB (default)│
│        Neo4j (optional)            Qdrant/Pinecone (opt)   │
└─────────────────────────────────────────────────────────────┘
```

---

## Core Domain Model

### 1. Node (The Only Entity Type)

```java
@Entity
@Table(schema = "graph", name = "nodes")
public class Node {
    @Id
    private UUID id;
    
    /**
     * User-defined label (e.g., "Patient", "Product", "Document")
     * Framework doesn't validate or constrain this
     */
    @Column(nullable = false)
    private String label;
    
    /**
     * Flexible property bag - framework agnostic
     * User stores whatever they need
     */
    @Column(type = "jsonb")
    private Map<String, Object> properties;
    
    /**
     * Optional: Text content for automatic vectorization
     * If provided, framework auto-generates embedding
     */
    @Column(type = "text")
    private String vectorizableContent;
    
    @Column(nullable = false)
    private Timestamp created;
    
    @Column(nullable = false)
    private Timestamp updated;
    
    @Version
    private Long version;
}
```

**No NodeType entity** - The framework doesn't enforce types. Users can use `label` however they want.

### 2. Edge (Relationships)

```java
@Entity
@Table(schema = "graph", name = "edges")
public class Edge {
    @Id
    private UUID id;
    
    /**
     * User-defined relation type (e.g., "TREATS", "CONTAINS", "IS_A")
     * Must be registered with framework via initialize()
     */
    @Column(nullable = false)
    private String relationType;
    
    @ManyToOne
    @JoinColumn(name = "source_node_id", nullable = false)
    private UUID sourceNodeId;
    
    @ManyToOne
    @JoinColumn(name = "target_node_id", nullable = false)
    private UUID targetNodeId;
    
    /**
     * Optional properties for the relationship
     */
    @Column(type = "jsonb")
    private Map<String, Object> properties;
    
    /**
     * Optional weight for weighted graph algorithms
     */
    private Double weight;
    
    @Column(nullable = false)
    private Timestamp created;
}
```

### 3. RelationType (Metadata - User Must Define)

```java
@Entity
@Table(schema = "graph", name = "relation_types")
public class RelationType {
    @Id
    private String name;
    
    /**
     * Semantic category for reasoning
     * Framework uses this to understand relationship semantics
     */
    @Enumerated(EnumType.STRING)
    private RelationSemantics semantics;
    
    /**
     * Is this bidirectional?
     */
    private boolean bidirectional;
    
    /**
     * Optional description for documentation
     */
    private String description;
    
    /**
     * User-defined metadata
     */
    @Column(type = "jsonb")
    private Map<String, Object> metadata;
}
```

### 4. RelationSemantics (Framework-Provided Enum)

```java
/**
 * Core relationship semantics that framework understands for reasoning.
 * User MUST map their domain relationships to these semantics.
 */
public enum RelationSemantics {
    /**
     * Hierarchical: parent-child relationship
     * Examples: "IS_CHILD_OF", "BELONGS_TO", "PART_OF"
     * Reasoning: Traverse up for context, down for details
     */
    HIERARCHY,
    
    /**
     * Containment: whole-part relationship
     * Examples: "CONTAINS", "HAS_COMPONENT", "INCLUDES"
     * Reasoning: Aggregate contained items for context
     */
    CONTAINMENT,
    
    /**
     * Type/Classification: instance-class relationship
     * Examples: "IS_A", "TYPE_OF", "INSTANCE_OF"
     * Reasoning: Get sibling instances, parent class properties
     */
    CLASSIFICATION,
    
    /**
     * Association: peer-to-peer relationship
     * Examples: "RELATED_TO", "ASSOCIATED_WITH", "TREATS"
     * Reasoning: Find related peers for similar context
     */
    ASSOCIATION,
    
    /**
     * Sequence: ordered relationship
     * Examples: "NEXT", "PREVIOUS", "FOLLOWS"
     * Reasoning: Get sequential context (before/after)
     */
    SEQUENCE,
    
    /**
     * Reference: pointing to related information
     * Examples: "REFERS_TO", "CITES", "MENTIONS"
     * Reasoning: Follow references for supporting information
     */
    REFERENCE,
    
    /**
     * Custom: user-defined semantics
     * Framework treats as generic association
     */
    CUSTOM
}
```

### 5. Chunk (Vector Storage)

```java
@Entity
@Table(schema = "vector", name = "chunks")
public class Chunk {
    @Id
    private UUID id;
    
    @Column(type = "text", nullable = false)
    private String content;
    
    @Column(type = "vector(768)")  // Configurable dimension
    private float[] embedding;
    
    /**
     * Optional link to a graph node
     * Enables hybrid search: start from node, constrain to its chunks
     */
    @ManyToOne
    @JoinColumn(name = "linked_node_id")
    private UUID linkedNodeId;
    
    /**
     * User-defined metadata (source, author, date, etc.)
     */
    @Column(type = "jsonb")
    private Map<String, Object> metadata;
    
    @Column(nullable = false)
    private Timestamp created;
}
```

---

## Framework Interfaces

### 1. KnowledgeIndexer - For Adding/Updating Data

```java
/**
 * Indexing interface for adding/updating knowledge graph.
 * Every operation updates BOTH database and in-memory graph.
 */
public interface KnowledgeIndexer {
    
    // ==================== Initialization ====================
    
    /**
     * Initialize framework with root nodes and relation types.
     * Must be called before any other operation.
     * 
     * @param rootNodes At least one root node (ontology starting point)
     * @param relationTypes Domain relation types mapped to semantics
     */
    void initialize(List<NodeInput> rootNodes, List<RelationType> relationTypes);
    
    
    // ==================== Full Indexing ====================
    
    /**
     * Full index: Load complete knowledge graph
     * - Clears in-memory graph
     * - Loads all data from database into JGraphT
     * - Use on startup or after major data changes
     */
    IndexResult fullIndex();
    
    /**
     * Full index from data source (files, blob storage, etc.)
     * - Uses registered parsers to parse data
     * - Persists to database
     * - Loads into in-memory graph
     * 
     * @param dataSource Abstraction over filesystem, S3, MinIO, etc.
     */
    IndexResult fullIndex(DataSource dataSource);
    
    
    // ==================== Incremental Indexing ====================
    
    /**
     * Index single node.
     * Updates: Database + In-Memory Graph
     * Generates embedding if vectorizableContent provided.
     */
    Node indexNode(NodeInput input);
    
    /**
     * Index single edge.
     * Updates: Database + In-Memory Graph
     */
    Edge indexEdge(EdgeInput input);
    
    /**
     * Index document chunk.
     * Updates: Database (vector schema)
     * Generates embedding automatically.
     */
    Chunk indexChunk(ChunkInput input);
    
    /**
     * Batch indexing for performance
     */
    BatchIndexResult indexBatch(BatchInput input);
    
    /**
     * Update node properties.
     * Updates: Database + In-Memory Graph
     * Regenerates embedding if vectorizableContent changed.
     */
    Node updateNode(UUID nodeId, Map<String, Object> properties);
    
    /**
     * Delete node (and optionally cascade edges).
     * Updates: Database + In-Memory Graph
     */
    void deleteNode(UUID nodeId, boolean cascade);
    
    /**
     * Delete edge.
     * Updates: Database + In-Memory Graph
     */
    void deleteEdge(UUID edgeId);
}
```

### 2. KnowledgeRetriever - For Querying Data (MCP Tool)

```java
/**
 * Retrieval interface for querying knowledge graph.
 * Uses JGraphT for fast in-memory operations.
 * Primary method exposed as MCP tool for LLM integration.
 */
public interface KnowledgeRetriever {
    
    // ==================== Basic Retrieval ====================
    
    /**
     * Find node by ID (from in-memory graph - O(1) fast)
     */
    Optional<Node> findNode(UUID id);
    
    /**
     * Find nodes by label (from in-memory index - O(1))
     */
    List<Node> findNodesByLabel(String label);
    
    /**
     * Find nodes by property (from database - flexible JSONB queries)
     */
    List<Node> findNodesByProperty(String propertyPath, Object value);
    
    
    // ==================== Graph Navigation ====================
    
    /**
     * Get parent nodes (uses JGraphT + RelationSemantics)
     * Follows: HIERARCHY, CONTAINMENT, CLASSIFICATION
     */
    List<Node> getParents(UUID nodeId);
    
    /**
     * Get children nodes
     */
    List<Node> getChildren(UUID nodeId);
    
    /**
     * Get siblings (shares same parent via CLASSIFICATION)
     */
    List<Node> getSiblings(UUID nodeId);
    
    /**
     * Get neighbors (all connected nodes)
     */
    List<Node> getNeighbors(UUID nodeId, String... relationTypes);
    
    /**
     * Find shortest path (uses JGraphT Dijkstra algorithm)
     */
    Optional<GraphPath> findPath(UUID fromId, UUID toId);
    
    
    // ==================== Context Extraction ====================
    
    /**
     * Extract reasoning context around a node.
     * Uses JGraphT BFS/DFS to traverse graph.
     * 
     * Returns:
     * - Related nodes (within depth)
     * - Relationship paths
     * - Context keywords (for VSS enhancement)
     * - Semantic groups (parents, children, siblings, neighbors)
     */
    ReasoningContext extractContext(UUID nodeId, int depth, 
                                   Set<RelationSemantics> semantics);
    
    
    // ==================== Intelligent Search (MCP Tool) ====================
    
    /**
     * PRIMARY METHOD - Exposed as MCP Tool
     * 
     * Intelligent search with Vector-First Resolution:
     * 1. Resolve entry points from graphQuery using Vector Search
     * 2. Extract reasoning context from resolved nodes (JGraphT traversal)
     * 3. Enhance query with context keywords
     * 4. Perform Hybrid Search (graph-constrained VSS)
     * 
     * @param query Structured graph query from LLM
     * @return SearchResult with chunks, context, strategy used
     */
    @MCPTool(
        name = "retrieve_knowledge",
        description = "Retrieve relevant knowledge. MUST use Graph Schema to construct graphQuery."
    )
    SearchResult retrieve(GraphQuery query);
    
    
    // ==================== Direct Vector Search ====================
    
    /**
     * Pure vector similarity search (bypasses graph reasoning)
     */
    List<RankedChunk> vectorSearch(String query, int topK);
    
    /**
     * Pure BM25 full-text search
     */
    List<RankedChunk> fullTextSearch(String query, int topK);
}
```

---

## Input/Output Types

### Input Types (for Indexing)

```java
public record NodeInput(
    UUID id,                              // Optional: null = auto-generate
    String label,                         // Required: user-defined
    Map<String, Object> properties,       // Required: flexible bag
    String vectorizableContent            // Optional: auto-embeds if present
) {}

public record EdgeInput(
    UUID id,                              // Optional
    String relationType,                  // Required: must be registered
    UUID sourceNodeId,                    // Required
    UUID targetNodeId,                    // Required
    Map<String, Object> properties,       // Optional
    Double weight                         // Optional: for weighted graphs
) {}

public record ChunkInput(
    UUID id,                              // Optional
    String content,                       // Required
    UUID linkedNodeId,                    // Optional: link to graph node
    Map<String, Object> metadata          // Optional
) {}

public record BatchInput(
    List<NodeInput> nodes,
    List<EdgeInput> edges,
    List<ChunkInput> chunks
) {}
```

### Output Types (from Retrieval)

```java
public record GraphQuery(
    String naturalLanguageQuery,          // Required: Original user question
    List<ResolutionTarget> targets,       // Required: Vector-first entry points
    TraversalHint traversal,              // Optional: Reasoning path hints
    int minResults,                       // Fallback threshold (default: 3)
    int topK,                             // Max results (default: 10)
    boolean includeReasoningContext,      // Return context? (default: true)
    int maxTokenBudget,                   // Max tokens for context
    int maxGraphNodes                     // Max nodes to traverse
) {}

public record ResolutionTarget(
    String description,                   // Required: Text to embed for vector search
    String typeHint,                      // Optional: "Patient", "Condition" (from schema)
    int maxMatches                        // Optional: Max nodes to resolve (default: 1)
) {}

public record TraversalHint(
    List<String> relationTypes,           // Optional: Restrict traversal to these relations
    int depth,                            // Default: 2
    Set<RelationSemantics> semantics      // Optional: Filter by semantics
) {}

public record SearchResult(
    List<RankedChunk> results,            // Retrieved chunks with scores
    SearchStrategy strategy,              // HYBRID, VECTOR, or BM25
    ReasoningContext reasoningContext,    // Graph context (if used)
    String enhancedQuery,                 // Query + keywords
    long durationMs,                      // Execution time
    Map<String, Object> metadata
) {}

public record ReasoningContext(
    Node rootNode,                        // Starting node
    List<Node> relatedNodes,              // Nodes from traversal
    List<GraphPath> paths,                // Relationship paths
    Set<String> contextKeywords,          // Keywords for VSS
    Map<RelationSemantics, List<Node>> semanticGroups,  // Grouped
    int depth,
    Map<String, Object> metadata
) {}

public record RankedChunk(
    Chunk chunk,
    double score,
    Map<String, Object> explanation
) {}

public record GraphPath(
    List<UUID> nodeIds,
    List<String> relationTypes,
    double weight,
    int length
) {}
```

---

## JGraphT In-Memory Graph

### Core Implementation

```java
/**
 * In-memory graph using JGraphT for fast operations.
 * Always kept in sync with database.
 */
public class InMemoryKnowledgeGraph {
    
    // JGraphT graph: allows multiple edges, self-loops, weights
    private final Graph<UUID, Edge> graph;
    
    // Fast lookups
    private final Map<UUID, Node> nodeIndex;              // O(1) lookup
    private final Map<String, Set<UUID>> labelIndex;      // O(1) by label
    private final Map<String, RelationType> relationTypes;
    
    public InMemoryKnowledgeGraph() {
        this.graph = new DirectedWeightedPseudograph<>(Edge.class);
        this.nodeIndex = new ConcurrentHashMap<>();
        this.labelIndex = new ConcurrentHashMap<>();
        this.relationTypes = new ConcurrentHashMap<>();
    }
    
    // ==================== Indexing Operations ====================
    
    public void addNode(Node node) {
        graph.addVertex(node.getId());
        nodeIndex.put(node.getId(), node);
        labelIndex.computeIfAbsent(node.getLabel(), k -> ConcurrentHashMap.newKeySet())
                  .add(node.getId());
    }
    
    public void addEdge(Edge edge) {
        graph.addEdge(edge.getSourceNodeId(), edge.getTargetNodeId(), edge);
        if (edge.getWeight() != null) {
            graph.setEdgeWeight(edge, edge.getWeight());
        }
        
        // Handle bidirectional
        RelationType relType = relationTypes.get(edge.getRelationType());
        if (relType != null && relType.bidirectional()) {
            Edge reverse = createReverseEdge(edge);
            graph.addEdge(edge.getTargetNodeId(), edge.getSourceNodeId(), reverse);
        }
    }
    
    public void removeNode(UUID nodeId) {
        graph.removeVertex(nodeId);
        Node node = nodeIndex.remove(nodeId);
        if (node != null) {
            labelIndex.getOrDefault(node.getLabel(), Set.of()).remove(nodeId);
        }
    }
    
    // ==================== Fast Retrieval ====================
    
    public Optional<Node> findNode(UUID id) {
        return Optional.ofNullable(nodeIndex.get(id));
    }
    
    public List<Node> findNodesByLabel(String label) {
        return labelIndex.getOrDefault(label, Set.of()).stream()
            .map(nodeIndex::get)
            .filter(Objects::nonNull)
            .toList();
    }
    
    public List<Node> getParents(UUID nodeId) {
        return graph.incomingEdgesOf(nodeId).stream()
            .filter(edge -> isParentSemantics(edge.getRelationType()))
            .map(Edge::getSourceNodeId)
            .map(nodeIndex::get)
            .filter(Objects::nonNull)
            .toList();
    }
    
    public List<Node> getChildren(UUID nodeId) {
        return graph.outgoingEdgesOf(nodeId).stream()
            .filter(edge -> isParentSemantics(edge.getRelationType()))
            .map(Edge::getTargetNodeId)
            .map(nodeIndex::get)
            .filter(Objects::nonNull)
            .toList();
    }
    
    public Optional<GraphPath> findShortestPath(UUID from, UUID to) {
        DijkstraShortestPath<UUID, Edge> dijkstra = new DijkstraShortestPath<>(graph);
        org.jgrapht.GraphPath<UUID, Edge> path = dijkstra.getPath(from, to);
        return path != null ? Optional.of(convertPath(path)) : Optional.empty();
    }
    
    // ==================== Graph Traversal (for Reasoning) ====================
    
    public ReasoningContext traverse(UUID startId, int depth, 
                                    Set<RelationSemantics> semanticsFilter) {
        Node rootNode = nodeIndex.get(startId);
        if (rootNode == null) {
            throw new NodeNotFoundException(startId);
        }
        
        Set<Node> visitedNodes = new HashSet<>();
        List<GraphPath> paths = new ArrayList<>();
        
        // BFS with depth limit
        Queue<TraversalState> queue = new LinkedList<>();
        queue.offer(new TraversalState(startId, 0, new ArrayList<>()));
        Set<UUID> visited = new HashSet<>();
        
        while (!queue.isEmpty()) {
            TraversalState state = queue.poll();
            
            if (state.depth > depth || !visited.add(state.nodeId)) {
                continue;
            }
            
            Node current = nodeIndex.get(state.nodeId);
            if (current != null) {
                visitedNodes.add(current);
            }
            
            // Traverse edges
            for (Edge edge : graph.outgoingEdgesOf(state.nodeId)) {
                if (!matchesSemantics(edge, semanticsFilter)) {
                    continue;
                }
                
                UUID nextId = edge.getTargetNodeId();
                List<Edge> newPath = new ArrayList<>(state.path);
                newPath.add(edge);
                paths.add(createGraphPath(newPath));
                
                if (state.depth < depth) {
                    queue.offer(new TraversalState(nextId, state.depth + 1, newPath));
                }
            }
        }
        
        // Extract keywords
        Set<String> keywords = extractKeywords(visitedNodes);
        
        // Group by semantics
        Map<RelationSemantics, List<Node>> groups = groupBySemantics(rootNode, visitedNodes);
        
        return new ReasoningContext(
            rootNode, 
            new ArrayList<>(visitedNodes), 
            paths, 
            keywords, 
            groups, 
            depth,
            Map.of("nodes_visited", visitedNodes.size())
        );
    }
    
    private record TraversalState(UUID nodeId, int depth, List<Edge> path) {}
    
    private boolean isParentSemantics(String relationType) {
        RelationType rt = relationTypes.get(relationType);
        return rt != null && (
            rt.semantics() == RelationSemantics.HIERARCHY ||
            rt.semantics() == RelationSemantics.CONTAINMENT ||
            rt.semantics() == RelationSemantics.CLASSIFICATION
        );
    }
}
```

### JGraphT Dependencies

```xml
<dependency>
    <groupId>org.jgrapht</groupId>
    <artifactId>jgrapht-core</artifactId>
    <version>1.5.2</version>
</dependency>
```

---

## Dual Persistence Strategy

Every indexing operation updates **both** database and in-memory graph:

```java
@Service
public class KnowledgeIndexerImpl implements KnowledgeIndexer {
    
    private final InMemoryKnowledgeGraph memoryGraph;
    private final NodeRepository nodeRepository;
    private final EdgeRepository edgeRepository;
    
    @Override
    @Transactional
    public Node indexNode(NodeInput input) {
        // 1. Create entity
        Node node = buildNode(input);
        
        // 2. Persist to database (transaction)
        node = nodeRepository.save(node);
        
        // 3. Update in-memory graph
        memoryGraph.addNode(node);
        
        // 4. Generate embedding if needed
        if (input.vectorizableContent() != null) {
            Chunk chunk = createChunk(input.vectorizableContent(), node.getId());
            chunkRepository.save(chunk);
        }
        
        return node;
    }
    
    @Override
    @Transactional
    public Edge indexEdge(EdgeInput input) {
        // 1. Validate relation type
        validateRelationType(input.relationType());
        
        // 2. Create entity
        Edge edge = buildEdge(input);
        
        // 3. Persist to database
        edge = edgeRepository.save(edge);
        
        // 4. Update in-memory graph
        memoryGraph.addEdge(edge);
        
        return edge;
    }
    
    @Override
    @Transactional
    public void deleteNode(UUID nodeId, boolean cascade) {
        // 1. Delete from database
        if (cascade) {
            edgeRepository.deleteBySourceOrTarget(nodeId);
        }
        nodeRepository.deleteById(nodeId);
        
        // 2. Delete from in-memory
        memoryGraph.removeNode(nodeId);
    }
    
    @Override
    public IndexResult fullIndex() {
        // 1. Clear in-memory
        memoryGraph.clear();
        
        // 2. Load from database
        List<Node> nodes = nodeRepository.findAll();
        nodes.forEach(memoryGraph::addNode);
        
        List<Edge> edges = edgeRepository.findAll();
        edges.forEach(memoryGraph::addEdge);
        
        return new IndexResult(nodes.size(), edges.size(), ...);
    }
}
```

---
    @Override
    @MCPTool(name = "retrieve_knowledge")
    public SearchResult retrieve(GraphQuery query) {
        long start = System.currentTimeMillis();
        
        // Step 1: Vector-First Resolution
        Set<UUID> startNodeIds = resolveTargets(query.targets());
        
        // Step 2: Extract context from graph (JGraphT)
        ReasoningContext context = null;
        if (!startNodeIds.isEmpty()) {
            context = memoryGraph.traverseMultiple(
                startNodeIds,
                query.traversal().depth(),
                query.traversal().semantics()
            );
        }
        
        // Step 3: Enhance query with keywords
        String enhanced = enhanceQuery(query.naturalLanguageQuery(), 
            context != null ? context.contextKeywords() : Set.of());
        
        // Step 4: Try Hybrid (graph + vector)
        if (context != null) {
            SearchResult result = hybridSearch(context, enhanced, query.topK());
            if (result.results().size() >= query.minResults()) {
                return enrichResult(result, SearchStrategy.HYBRID, context, enhanced, start);
            }
        }
        
        // Step 5: Fallback to VSS
        SearchResult result = vectorSearch(enhanced, query.topK());
        if (result.results().size() >= query.minResults()) {
            return enrichResult(result, SearchStrategy.VECTOR, context, enhanced, start);
        }
        
        // Step 6: Fallback to BM25
        result = bm25Search(query.naturalLanguageQuery(), query.topK());
        return enrichResult(result, SearchStrategy.BM25, context, query.naturalLanguageQuery(), start);
    }
    
    private Set<UUID> resolveTargets(List<ResolutionTarget> targets) {
        // Implementation: Embed description, find nearest nodes
        return targets.stream()
            .flatMap(t -> findNodesByVector(t.description(), t.typeHint(), t.maxMatches()).stream())
            .map(Node::getId)
            .collect(Collectors.toSet());
    }
    
    private SearchResult hybridSearch(ReasoningContext ctx, String query, int topK) {
        // Get chunks linked to nodes in reasoning context
        Set<UUID> contextChunkIds = ctx.relatedNodes().stream()
            .flatMap(n -> chunkRepository.findByLinkedNodeId(n.getId()).stream())
            .map(Chunk::getId)
            .collect(Collectors.toSet());
        
        if (contextChunkIds.isEmpty()) {
            return new SearchResult(List.of(), SearchStrategy.HYBRID, ...);
        }
        
        // Vector search constrained to these chunks
        float[] embedding = embeddingProvider.embed(query);
        List<RankedChunk> results = chunkRepository.vectorSearchFiltered(
            embedding, contextChunkIds, topK
        );
        
        return new SearchResult(results, SearchStrategy.HYBRID, ...);
    }
}
```

---

## Pluggable Storage Abstraction

### GraphStore Interface

**Decision:** Abstract graph storage to support multiple backends (JGraphT, Neo4j, TinkerPop).

```java
/**
 * Abstraction for graph storage implementations.
 * Framework provides JGraphT default, users can plug in Neo4j, TinkerPop, etc.
 */
public interface GraphStore {
    
    // ==================== Indexing Operations ====================
    
    void addNode(Node node);
    void addEdge(Edge edge);
    void removeNode(UUID nodeId);
    void removeEdge(UUID edgeId);
    void clear();
    
    // ==================== Retrieval Operations ====================
    
    Optional<Node> findNode(UUID id);
    List<Node> findNodesByLabel(String label);
    
    // ==================== Graph Navigation ====================
    
    List<Node> getParents(UUID nodeId);
    List<Node> getChildren(UUID nodeId);
    List<Node> getNeighbors(UUID nodeId, String... relationTypes);
    
    // ==================== Graph Traversal ====================
    
    /**
     * Traverse graph from startNode with depth limit.
     * Returns reasoning context with related nodes and paths.
     */
    ReasoningContext traverse(UUID startId, int depth, Set<RelationSemantics> semanticsFilter);
    
    /**
     * Find shortest path between two nodes.
     */
    Optional<GraphPath> findPath(UUID from, UUID to);
    
    // ==================== Metadata ====================
    
    long getNodeCount();
    long getEdgeCount();
    String getImplementation();  // "jgrapht", "neo4j", "tinkerpop"
}
```

### Implementations

```java
/**
 * JGraphT implementation (default) - optimized for <100K nodes
 */
public class JGraphTGraphStore implements GraphStore {
    private final Graph<UUID, Edge> graph;
    private final Map<UUID, Node> nodeIndex;
    private final Map<String, Set<UUID>> labelIndex;
    
    // Implementation using existing InMemoryKnowledgeGraph logic
}

/**
 * Neo4j implementation (optional) - for millions of nodes
 */
public class Neo4jGraphStore implements GraphStore {
    private final Driver neo4jDriver;
    
    @Override
    public ReasoningContext traverse(UUID startId, int depth, Set<RelationSemantics> semantics) {
        // Use Cypher queries for traversal
        String cypher = "MATCH path = (start:Node {id: $startId})-[*1.." + depth + "]->(related) RETURN path";
        // ... execute and convert to ReasoningContext
    }
}

/**
 * TinkerPop/Gremlin implementation (optional) - for Apache TinkerGraph, JanusGraph, etc.
 */
public class TinkerPopGraphStore implements GraphStore {
    private final GraphTraversalSource g;
    
    @Override
    public ReasoningContext traverse(UUID startId, int depth, Set<RelationSemantics> semantics) {
        // Use Gremlin traversal
        // g.V().has("id", startId).repeat(__.out()).times(depth)
    }
}
```

### VectorStore Interface

**Decision:** Abstract vector storage to support multiple backends (Postgres/pgvector, DuckDB, Qdrant, Pinecone).

```java
/**
 * Abstraction for vector storage implementations.
 */
public interface VectorStore {
    
    // ==================== Indexing ====================
    
    void indexChunk(Chunk chunk);
    void indexChunkBatch(List<Chunk> chunks);
    void deleteChunk(UUID chunkId);
    
    // ==================== Search ====================
    
    /**
     * Vector similarity search.
     */
    List<RankedChunk> vectorSearch(float[] embedding, int topK);
    
    /**
     * Vector search filtered to specific chunks.
     */
    List<RankedChunk> vectorSearchFiltered(float[] embedding, Set<UUID> chunkIds, int topK);
    
    /**
     * BM25 full-text search.
     */
    List<RankedChunk> fullTextSearch(String query, int topK);
    
    // ==================== Metadata ====================
    
    long getChunkCount();
    String getImplementation();  // "postgres", "duckdb", "qdrant", "pinecone"
}
```

### Implementations

```java
/**
 * PostgreSQL + pgvector implementation (default)
 */
public class PostgresVectorStore implements VectorStore {
    private final ChunkRepository chunkRepository;
    
    @Override
    public List<RankedChunk> vectorSearch(float[] embedding, int topK) {
        // Use pgvector <-> operator
        return chunkRepository.findNearestNeighbors(embedding, topK);
    }
}

/**
 * DuckDB VSS implementation (lightweight alternative)
 */
public class DuckDBVectorStore implements VectorStore {
    private final Connection duckDbConnection;
    
    @Override
    public List<RankedChunk> vectorSearch(float[] embedding, int topK) {
        // Use DuckDB VSS extension
        String sql = "SELECT * FROM chunks ORDER BY array_distance(embedding, ?::FLOAT[]) LIMIT ?";
        // ... execute and convert
    }
}

/**
 * Qdrant implementation (optional, specialized vector DB)
 */
public class QdrantVectorStore implements VectorStore {
    private final QdrantClient client;
    
    @Override
    public List<RankedChunk> vectorSearch(float[] embedding, int topK) {
        SearchRequest request = SearchRequest.newBuilder()
            .setCollectionName("chunks")
            .addAllVector(Arrays.asList(embedding))
            .setLimit(topK)
            .build();
        return client.search(request);
    }
}

/**
 * Pinecone implementation (optional, cloud vector DB)
 */
public class PineconeVectorStore implements VectorStore {
    private final PineconeClient client;
    
    @Override
    public List<RankedChunk> vectorSearch(float[] embedding, int topK) {
        QueryRequest request = new QueryRequest()
            .vector(embedding)
            .topK(topK)
            .includeMetadata(true);
        return client.query(request);
    }
}
```

### Configuration

```yaml
cef:
  graph:
    store: jgrapht  # jgrapht | neo4j | tinkerpop
    neo4j:
      uri: bolt://localhost:7687
      username: neo4j
      password: password
    jgrapht:
      max-nodes: 100000
      preload-on-startup: true
  
  vector:
    store: postgres  # postgres | duckdb | qdrant | pinecone
    postgres:
      # Uses spring.datasource config
    duckdb:
      path: ./data/knowledge.duckdb
    qdrant:
      host: localhost
      port: 6333
      api-key: ${QDRANT_API_KEY}
    pinecone:
      api-key: ${PINECONE_API_KEY}
      environment: us-east-1
      index-name: cef-chunks
```

---

## Parser System (Optional ANTLR for Complex Documents)LR for Complex Documents)

**Decision:** Simple file formats (YAML/CSV/JSON) use standard libraries. ANTLR only for complex custom document formats (PDFs with specific structure, proprietary formats).

**Use Cases for ANTLR:**
- Custom PDF layouts (10-50 page medical reports, legal contracts)
- Domain-specific languages (custom config formats, query languages)
- Proprietary document formats with defined grammar

**Use Standard Libraries for:**
- YAML/JSON/CSV (SnakeYAML, Jackson, OpenCSV)
- Plain text documents
- HTML/XML (Jsoup, standard XML parsers)

### AbstractParser - Base Parser Interface

```java
/**
 * Abstract parser for data sources.
 * Framework provides this, users extend for their domain.
 * ANTLR is OPTIONAL - only use for complex custom formats.
 */
public abstract class AbstractParser<T extends ParsedData> {
    
    /**
     * Supported file extensions or content types
     * Examples: [".yml", ".yaml"], [".csv"], [".json"], [".xml"]
     */
    public abstract List<String> getSupportedExtensions();
    
    /**
     * Parser priority/order (lower = higher priority)
     * Used when multiple parsers support same extension
     */
    public abstract int getOrder();
    
    /**
     * Parse data from input stream
     * 
     * @param input Input stream from data source
     * @param context Parsing context (file path, metadata, etc.)
     * @return Parsed data ready for indexing
     * @throws ParseException if parsing fails
     */
    public abstract T parse(InputStream input, ParseContext context) 
        throws ParseException;
    
    /**
     * Validate parsed data (optional override)
     */
    public void validate(T data) throws ValidationException {
        // Default: no validation
    }
    
    /**
     * Extract nodes from parsed data
     */
    public abstract List<NodeInput> extractNodes(T data);
    
    /**
     * Extract edges from parsed data
     */
    public abstract List<EdgeInput> extractEdges(T data);
    
    /**
     * Extract chunks from parsed data (documents, descriptions)
     */
    public abstract List<ChunkInput> extractChunks(T data);
}
```

### ParsedData - Base Data Structure

```java
/**
 * Base class for parsed data.
 * Users extend this for their domain-specific parsed structures.
 */
public abstract class ParsedData {
    private final String source;          // File path or URL
    private final String format;          // File format (yaml, csv, json)
    private final Map<String, Object> metadata;
    private final Timestamp parsedAt;
    
    protected ParsedData(String source, String format) {
        this.source = source;
        this.format = format;
        this.metadata = new HashMap<>();
        this.parsedAt = Timestamp.now();
    }
    
    // Getters and metadata manipulation
    public void addMetadata(String key, Object value) {
        metadata.put(key, value);
    }
}
```

### ParseContext - Parsing Context

```java
/**
 * Context information for parsing
 */
public record ParseContext(
    String sourcePath,                    // File path or URL
    String fileName,                      // File name
    String extension,                     // File extension
    Map<String, Object> metadata,         // Additional metadata
    DataSource dataSource                 // Source adapter
) {}
```

### ParserFactory - Parser Registry & Selection

```java
/**
 * Factory for managing and selecting parsers.
 * Framework provides this, users register their parsers.
 */
@Component
public class ParserFactory {
    
    private final List<AbstractParser<?>> parsers = new CopyOnWriteArrayList<>();
    
    /**
     * Register a parser with the factory.
     * Called by users in their @Configuration class.
     */
    public void registerParser(AbstractParser<?> parser) {
        parsers.add(parser);
        // Sort by order (lower = higher priority)
        parsers.sort(Comparator.comparingInt(AbstractParser::getOrder));
        
        log.info("Registered parser: {} for extensions: {} with order: {}",
                parser.getClass().getSimpleName(),
                parser.getSupportedExtensions(),
                parser.getOrder());
    }
    
    /**
     * Find parser for file extension.
     * Returns highest priority parser (lowest order number).
     */
    public Optional<AbstractParser<?>> findParser(String extension) {
        return parsers.stream()
            .filter(p -> p.getSupportedExtensions().contains(extension))
            .findFirst();  // Already sorted by order
    }
    
    /**
     * Get all registered parsers
     */
    public List<AbstractParser<?>> getAllParsers() {
        return Collections.unmodifiableList(parsers);
    }
    
    /**
     * Parse file using appropriate parser
     */
    public ParsedData parse(InputStream input, ParseContext context) 
        throws ParseException {
        
        AbstractParser<?> parser = findParser(context.extension())
            .orElseThrow(() -> new ParseException(
                "No parser found for extension: " + context.extension()));
        
        return parser.parse(input, context);
    }
}
```

### Built-in Parsers (Framework Provides)

```java
/**
 * YAML parser using ANTLR and SnakeYAML
 * Framework provides this for common formats
 */
@Component
public class YamlParser extends AbstractParser<YamlParsedData> {
    
    @Override
    public List<String> getSupportedExtensions() {
        return List.of(".yml", ".yaml");
    }
    
    @Override
    public int getOrder() {
        return 100;  // Default priority
    }
    
    @Override
    public YamlParsedData parse(InputStream input, ParseContext context) 
        throws ParseException {
        try {
            Yaml yaml = new Yaml();
            Map<String, Object> data = yaml.load(input);
            return new YamlParsedData(context.sourcePath(), data);
        } catch (Exception e) {
            throw new ParseException("Failed to parse YAML: " + context.sourcePath(), e);
        }
    }
    
    @Override
    public List<NodeInput> extractNodes(YamlParsedData data) {
        // Default implementation: look for "nodes" key
        List<Map<String, Object>> nodesList = 
            (List<Map<String, Object>>) data.getRawData().get("nodes");
        
        if (nodesList == null) {
            return List.of();
        }
        
        return nodesList.stream()
            .map(this::mapToNodeInput)
            .toList();
    }
    
    @Override
    public List<EdgeInput> extractEdges(YamlParsedData data) {
        // Default implementation: look for "edges" key
        // ... similar to extractNodes
    }
    
    @Override
    public List<ChunkInput> extractChunks(YamlParsedData data) {
        // Default implementation: look for "documents" or "chunks" key
        // ... similar to extractNodes
    }
}

/**
 * CSV parser using ANTLR and OpenCSV
 */
@Component
public class CsvParser extends AbstractParser<CsvParsedData> {
    
    @Override
    public List<String> getSupportedExtensions() {
        return List.of(".csv");
    }
    
    @Override
    public int getOrder() {
        return 100;
    }
    
    @Override
    public CsvParsedData parse(InputStream input, ParseContext context) 
        throws ParseException {
        try (CSVReader reader = new CSVReader(new InputStreamReader(input))) {
            List<String[]> rows = reader.readAll();
            return new CsvParsedData(context.sourcePath(), rows);
        } catch (Exception e) {
            throw new ParseException("Failed to parse CSV: " + context.sourcePath(), e);
        }
    }
    
    // ... extract methods
}

/**
 * JSON parser using Jackson (NOT ANTLR - simple library sufficient)
 */
@Component
public class JsonParser extends AbstractParser<JsonParsedData> {
    
    private final ObjectMapper objectMapper;
    
    @Override
    public List<String> getSupportedExtensions() {
        return List.of(".json");
    }
    
    @Override
    public int getOrder() {
        return 100;
    }
    
    @Override
    public JsonParsedData parse(InputStream input, ParseContext context) 
        throws ParseException {
        try {
            JsonNode root = objectMapper.readTree(input);
            return new JsonParsedData(context.sourcePath(), root);
        } catch (Exception e) {
            throw new ParseException("Failed to parse JSON: " + context.sourcePath(), e);
        }
    }
    
    // ... extract methods
}
```

### Example: ANTLR Parser for Complex PDF (User's Custom Implementation)

**Use Case:** Medical lab reports with 10-50 pages, specific section structure.

```java
/**
 * ANTLR-based parser for structured medical PDFs.
 * User creates grammar for their specific document format.
 * 
 * Grammar file (LabReport.g4):
 * 
 * labReport : header sections footer ;
 * header : PATIENT_ID NAME DOB ;
 * sections : testSection+ ;
 * testSection : TEST_NAME results ;
 * results : (TEST_RESULT UNIT RANGE)+ ;
 */
@Component
public class MedicalPdfParser extends AbstractParser<LabReportData> {
    
    @Override
    public List<String> getSupportedExtensions() {
        return List.of(".pdf");
    }
    
    @Override
    public int getOrder() {
        return 10;  // Higher priority than generic PDF parser
    }
    
    @Override
    public LabReportData parse(InputStream input, ParseContext context) 
        throws ParseException {
        try {
            // Step 1: Extract text from PDF
            String pdfText = extractTextFromPdf(input);
            
            // Step 2: Parse with ANTLR
            LabReportLexer lexer = new LabReportLexer(CharStreams.fromString(pdfText));
            CommonTokenStream tokens = new CommonTokenStream(lexer);
            LabReportParser parser = new LabReportParser(tokens);
            
            // Step 3: Walk parse tree with custom visitor
            LabReportVisitor visitor = new LabReportVisitor();
            LabReportData data = visitor.visit(parser.labReport());
            
            return data;
        } catch (Exception e) {
            throw new ParseException("Failed to parse medical PDF: " + context.sourcePath(), e);
        }
    }
    
    @Override
    public List<NodeInput> extractNodes(LabReportData data) {
        List<NodeInput> nodes = new ArrayList<>();
        
        // Create patient node
        nodes.add(new NodeInput(
            UUID.fromString(data.getPatientId()),
            "Patient",
            Map.of("name", data.getPatientName(), "dob", data.getDob()),
            null
        ));
        
        // Create test result nodes
        for (TestResult result : data.getTestResults()) {
            nodes.add(new NodeInput(
                null,
                "TestResult",
                Map.of(
                    "testName", result.getName(),
                    "value", result.getValue(),
                    "unit", result.getUnit(),
                    "normalRange", result.getRange()
                ),
                result.getInterpretation()  // Vectorizable content
            ));
        }
        
        return nodes;
    }
    
    @Override
    public List<EdgeInput> extractEdges(LabReportData data) {
        // Create HAS_TEST_RESULT edges between patient and results
        // ...
    }
}

/**
 * Custom ANTLR visitor for lab report grammar
 */
class LabReportVisitor extends LabReportBaseVisitor<LabReportData> {
    
    @Override
    public LabReportData visitLabReport(LabReportParser.LabReportContext ctx) {
        LabReportData data = new LabReportData();
        
        // Visit header
        LabReportParser.HeaderContext header = ctx.header();
        data.setPatientId(header.PATIENT_ID().getText());
        data.setPatientName(header.NAME().getText());
        data.setDob(header.DOB().getText());
        
        // Visit test sections
        for (LabReportParser.TestSectionContext section : ctx.sections().testSection()) {
            TestResult result = new TestResult();
            result.setName(section.TEST_NAME().getText());
            
            // Parse results
            for (int i = 0; i < section.results().TEST_RESULT().size(); i++) {
                result.setValue(section.results().TEST_RESULT(i).getText());
                result.setUnit(section.results().UNIT(i).getText());
                result.setRange(section.results().RANGE(i).getText());
            }
            
            data.addTestResult(result);
        }
        
        return data;
    }
}
```

**Summary of ANTLR Usage:**
- ✅ Use ANTLR for complex, structured documents (10-50 page PDFs, proprietary formats)
- ✅ Define grammar once, parse many documents efficiently
- ✅ Better than regex/string manipulation for structured formats
- ❌ Don't use ANTLR for simple YAML/JSON/CSV (standard libraries sufficient)
- ❌ Don't use ANTLR for unstructured text (use NLP libraries)

### Example: User's Simple Custom Parser (No ANTLR Needed)

```java
/**
 * User implements custom parser for their domain
 * This lives in the example project, not the framework
 */
@Component
public class MedicalYamlParser extends AbstractParser<MedicalParsedData> {
    
    @Override
    public List<String> getSupportedExtensions() {
        return List.of(".yml", ".yaml");
    }
    
    @Override
    public int getOrder() {
        return 10;  // Higher priority than default YamlParser (100)
    }
    
    @Override
    public MedicalParsedData parse(InputStream input, ParseContext context) 
        throws ParseException {
        Yaml yaml = new Yaml();
        Map<String, Object> data = yaml.load(input);
        
        // Custom parsing logic for medical domain
        return new MedicalParsedData(context.sourcePath(), data);
    }
    
    @Override
    public List<NodeInput> extractNodes(MedicalParsedData data) {
        List<NodeInput> nodes = new ArrayList<>();
        
        // Extract patients
        List<Map<String, Object>> patients = data.getPatients();
        for (Map<String, Object> patient : patients) {
            nodes.add(new NodeInput(
                UUID.fromString((String) patient.get("id")),
                "Patient",
                patient,
                (String) patient.get("symptoms")  // Vectorizable content
            ));
        }
        
        // Extract doctors, conditions, etc.
        // ... domain-specific logic
        
        return nodes;
    }
    
    @Override
    public List<EdgeInput> extractEdges(MedicalParsedData data) {
        // Extract relationships from medical data
        // ... domain-specific logic
    }
    
    @Override
    public List<ChunkInput> extractChunks(MedicalParsedData data) {
        // Extract medical documents, guidelines, etc.
        // ... domain-specific logic
    }
}

/**
 * User registers their parser in configuration
 */
@Configuration
public class MedicalParserConfig {
    
    @Autowired
    private ParserFactory parserFactory;
    
    @Bean
    public MedicalYamlParser medicalYamlParser() {
        MedicalYamlParser parser = new MedicalYamlParser();
        parserFactory.registerParser(parser);
        return parser;
    }
}
```

---

## DataSource System

### DataSource - Abstraction Interface

```java
/**
 * Abstraction over data sources (filesystem, blob storage, etc.)
 * Framework provides this interface and common implementations.
 */
public interface DataSource {
    
    /**
     * List all files/resources in the data source
     */
    List<ResourceInfo> listResources() throws DataSourceException;
    
    /**
     * List resources matching pattern (glob or regex)
     */
    List<ResourceInfo> listResources(String pattern) throws DataSourceException;
    
    /**
     * Get input stream for a resource
     */
    InputStream getInputStream(String resourcePath) throws DataSourceException;
    
    /**
     * Check if resource exists
     */
    boolean exists(String resourcePath);
    
    /**
     * Get resource metadata
     */
    Optional<ResourceInfo> getResourceInfo(String resourcePath);
    
    /**
     * Close data source (cleanup connections)
     */
    void close() throws DataSourceException;
}
```

### ResourceInfo - Resource Metadata

```java
/**
 * Metadata about a resource (file, blob, etc.)
 */
public record ResourceInfo(
    String path,                          // Full path
    String name,                          // File/resource name
    String extension,                     // File extension
    long size,                            // Size in bytes
    Timestamp lastModified,               // Last modification time
    String contentType,                   // MIME type (if available)
    Map<String, Object> metadata          // Additional metadata
) {}
```

### FileSystemDataSource - Local Filesystem Adapter

```java
/**
 * Framework provides filesystem adapter
 */
@Component
public class FileSystemDataSource implements DataSource {
    
    private final Path rootPath;
    
    public FileSystemDataSource(String rootPath) {
        this.rootPath = Paths.get(rootPath);
        if (!Files.exists(this.rootPath)) {
            throw new IllegalArgumentException("Root path does not exist: " + rootPath);
        }
    }
    
    @Override
    public List<ResourceInfo> listResources() throws DataSourceException {
        try {
            return Files.walk(rootPath)
                .filter(Files::isRegularFile)
                .map(this::toResourceInfo)
                .toList();
        } catch (IOException e) {
            throw new DataSourceException("Failed to list resources", e);
        }
    }
    
    @Override
    public List<ResourceInfo> listResources(String pattern) throws DataSourceException {
        PathMatcher matcher = FileSystems.getDefault()
            .getPathMatcher("glob:" + pattern);
        
        try {
            return Files.walk(rootPath)
                .filter(Files::isRegularFile)
                .filter(p -> matcher.matches(rootPath.relativize(p)))
                .map(this::toResourceInfo)
                .toList();
        } catch (IOException e) {
            throw new DataSourceException("Failed to list resources with pattern", e);
        }
    }
    
    @Override
    public InputStream getInputStream(String resourcePath) throws DataSourceException {
        try {
            Path fullPath = rootPath.resolve(resourcePath);
            return Files.newInputStream(fullPath);
        } catch (IOException e) {
            throw new DataSourceException("Failed to open resource: " + resourcePath, e);
        }
    }
    
    @Override
    public boolean exists(String resourcePath) {
        return Files.exists(rootPath.resolve(resourcePath));
    }
    
    @Override
    public Optional<ResourceInfo> getResourceInfo(String resourcePath) {
        Path path = rootPath.resolve(resourcePath);
        return Files.exists(path) ? 
            Optional.of(toResourceInfo(path)) : 
            Optional.empty();
    }
    
    @Override
    public void close() {
        // Nothing to close for filesystem
    }
    
    private ResourceInfo toResourceInfo(Path path) {
        try {
            BasicFileAttributes attrs = Files.readAttributes(path, BasicFileAttributes.class);
            String fileName = path.getFileName().toString();
            String extension = getExtension(fileName);
            
            return new ResourceInfo(
                rootPath.relativize(path).toString(),
                fileName,
                extension,
                attrs.size(),
                Timestamp.from(attrs.lastModifiedTime().toInstant()),
                Files.probeContentType(path),
                Map.of()
            );
        } catch (IOException e) {
            throw new UncheckedIOException(e);
        }
    }
    
    private String getExtension(String fileName) {
        int lastDot = fileName.lastIndexOf('.');
        return lastDot > 0 ? fileName.substring(lastDot) : "";
    }
}
```

### BlobStorageDataSource - S3/MinIO Adapter

```java
/**
 * Framework provides blob storage adapter (S3-compatible)
 * Tested with MinIO
 */
@Component
public class BlobStorageDataSource implements DataSource {
    
    private final S3Client s3Client;
    private final String bucketName;
    private final String prefix;
    
    public BlobStorageDataSource(S3Client s3Client, String bucketName, String prefix) {
        this.s3Client = s3Client;
        this.bucketName = bucketName;
        this.prefix = prefix != null ? prefix : "";
    }
    
    /**
     * Builder for MinIO configuration
     */
    public static BlobStorageDataSource forMinIO(
        String endpoint,
        String accessKey,
        String secretKey,
        String bucketName,
        String prefix
    ) {
        S3Client s3Client = S3Client.builder()
            .endpointOverride(URI.create(endpoint))
            .credentialsProvider(StaticCredentialsProvider.create(
                AwsBasicCredentials.create(accessKey, secretKey)))
            .region(Region.US_EAST_1)  // MinIO doesn't care about region
            .serviceConfiguration(S3Configuration.builder()
                .pathStyleAccessEnabled(true)
                .build())
            .build();
        
        return new BlobStorageDataSource(s3Client, bucketName, prefix);
    }
    
    @Override
    public List<ResourceInfo> listResources() throws DataSourceException {
        try {
            ListObjectsV2Request request = ListObjectsV2Request.builder()
                .bucket(bucketName)
                .prefix(prefix)
                .build();
            
            List<ResourceInfo> resources = new ArrayList<>();
            ListObjectsV2Response response;
            
            do {
                response = s3Client.listObjectsV2(request);
                resources.addAll(response.contents().stream()
                    .map(this::toResourceInfo)
                    .toList());
                
                request = request.toBuilder()
                    .continuationToken(response.nextContinuationToken())
                    .build();
            } while (response.isTruncated());
            
            return resources;
        } catch (S3Exception e) {
            throw new DataSourceException("Failed to list S3 objects", e);
        }
    }
    
    @Override
    public List<ResourceInfo> listResources(String pattern) throws DataSourceException {
        // Use glob pattern to filter
        Pattern regex = convertGlobToRegex(pattern);
        return listResources().stream()
            .filter(r -> regex.matcher(r.path()).matches())
            .toList();
    }
    
    @Override
    public InputStream getInputStream(String resourcePath) throws DataSourceException {
        try {
            GetObjectRequest request = GetObjectRequest.builder()
                .bucket(bucketName)
                .key(prefix + resourcePath)
                .build();
            
            return s3Client.getObject(request);
        } catch (S3Exception e) {
            throw new DataSourceException("Failed to get S3 object: " + resourcePath, e);
        }
    }
    
    @Override
    public boolean exists(String resourcePath) {
        try {
            HeadObjectRequest request = HeadObjectRequest.builder()
                .bucket(bucketName)
                .key(prefix + resourcePath)
                .build();
            
            s3Client.headObject(request);
            return true;
        } catch (S3Exception e) {
            return false;
        }
    }
    
    @Override
    public Optional<ResourceInfo> getResourceInfo(String resourcePath) {
        try {
            HeadObjectRequest request = HeadObjectRequest.builder()
                .bucket(bucketName)
                .key(prefix + resourcePath)
                .build();
            
            HeadObjectResponse response = s3Client.headObject(request);
            return Optional.of(toResourceInfo(resourcePath, response));
        } catch (S3Exception e) {
            return Optional.empty();
        }
    }
    
    @Override
    public void close() {
        s3Client.close();
    }
    
    private ResourceInfo toResourceInfo(S3Object s3Object) {
        String key = s3Object.key();
        String name = key.substring(key.lastIndexOf('/') + 1);
        String extension = getExtension(name);
        
        return new ResourceInfo(
            key.startsWith(prefix) ? key.substring(prefix.length()) : key,
            name,
            extension,
            s3Object.size(),
            Timestamp.from(s3Object.lastModified()),
            null,  // Content type not available in list
            Map.of("etag", s3Object.eTag())
        );
    }
    
    private ResourceInfo toResourceInfo(String path, HeadObjectResponse response) {
        String name = path.substring(path.lastIndexOf('/') + 1);
        String extension = getExtension(name);
        
        return new ResourceInfo(
            path,
            name,
            extension,
            response.contentLength(),
            Timestamp.from(response.lastModified()),
            response.contentType(),
            Map.of("etag", response.eTag())
        );
    }
}
```

### DataSourceFactory - Factory for DataSources

```java
/**
 * Factory for creating data sources from configuration
 */
@Component
public class DataSourceFactory {
    
    public DataSource createFromConfig(DataSourceConfig config) {
        return switch (config.getType()) {
            case FILESYSTEM -> new FileSystemDataSource(config.getPath());
            case S3 -> createS3DataSource(config);
            case MINIO -> createMinIODataSource(config);
            default -> throw new IllegalArgumentException("Unknown data source type: " + config.getType());
        };
    }
    
    private DataSource createS3DataSource(DataSourceConfig config) {
        S3Client s3Client = S3Client.builder()
            .region(Region.of(config.getRegion()))
            .credentialsProvider(DefaultCredentialsProvider.create())
            .build();
        
        return new BlobStorageDataSource(
            s3Client,
            config.getBucket(),
            config.getPrefix()
        );
    }
    
    private DataSource createMinIODataSource(DataSourceConfig config) {
        return BlobStorageDataSource.forMinIO(
            config.getEndpoint(),
            config.getAccessKey(),
            config.getSecretKey(),
            config.getBucket(),
            config.getPrefix()
        );
    }
}
```

---

## MCP Tool System & Schema Discovery

### Tool Format Configuration

**Decision:** Support both OpenAI Function Calling and MCP format via configuration. Developer controls field requirements.

**Configuration:**
```yaml
cef:
  mcp:
    tool-format: openai  # openai | mcp (default: openai)
    schema-injection: dynamic  # static | dynamic (default: dynamic)
    required-fields:
      text-query: true  # Always required
      graph-hints: false  # optional | required (default: optional)
      semantic-keywords: false  # optional | required (default: optional)
```

**Philosophy:** With proper schema + examples in tool description, LLMs construct correct graphHints. Framework trusts LLM intelligence. Developer can enforce requirements per use case.

**OpenAI Format (Default - Configurable Requirements):**
```json
{
  "type": "function",
  "function": {
    "name": "retrieve_context",
    "description": "Retrieve context using graph reasoning + semantic search",
    "parameters": {
      "type": "object",
      "properties": {
        "textQuery": {"type": "string", "description": "Natural language query"},
        "graphHints": {
          "type": "object",
          "description": "Graph traversal hints - provide when entity mentioned in query",
          "properties": {
            "entityLabel": {"type": "string", "description": "Node type from schema (e.g., Patient, Doctor)"},
            "entityProperty": {"type": "string", "description": "Property to filter (e.g., name, id)"},
            "entityValue": {"description": "Value to match (e.g., 'John', 12345)"},
            "traverseRelations": {"type": "array", "items": {"type": "string"}, "description": "Relations to traverse from schema"},
            "depth": {"type": "integer", "default": 2, "description": "Traversal depth (1-5)"}
          },
          "required": ["entityLabel", "entityProperty", "entityValue"]
        },
        "semanticKeywords": {"type": "array", "items": {"type": "string"}, "description": "Keywords to boost semantic search"}
      },
      "required": ["textQuery"]  // graphHints, semanticKeywords added based on config
    }
  }
}

// Framework dynamically adjusts 'required' array based on cef.mcp.required-fields config
```

**MCP Format (Alternative - Configurable Requirements):**
```json
{
  "name": "retrieve_context",
  "description": "Retrieve context using graph reasoning + semantic search",
  "inputSchema": {
    "type": "object",
    "properties": {
      "textQuery": {"type": "string", "description": "Natural language query"},
      "graphHints": {
        "type": "object",
        "description": "Optional graph hints - use when entity mentioned",
        "properties": {
          "entityLabel": {"type": "string"},
          "entityProperty": {"type": "string"},
          "entityValue": {},
          "traverseRelations": {"type": "array", "items": {"type": "string"}},
          "depth": {"type": "integer", "default": 2}
        },
        "required": ["entityLabel", "entityProperty", "entityValue"]
      },
      "semanticKeywords": {"type": "array", "items": {"type": "string"}}
    },
    "required": ["textQuery"]  // Dynamically extended based on config
  }
}

// Framework reads cef.mcp.required-fields and adjusts schema at startup
```

### Dynamic Schema Injection

**Schema is dynamically injected into tool description at runtime:**

```java
@Component
public class MCPToolRegistry {
    private final GraphSchemaRegistry schemaRegistry;
    
    @PostConstruct
    public void registerTools() {
        GraphSchema schema = schemaRegistry.getSchema();
        String schemaDescription = formatSchemaForLLM(schema);
        
        // Inject into tool description
        String description = """
            Retrieve context using graph reasoning + semantic search.
            
            GRAPH SCHEMA:
            %s
            
            REQUEST SCHEMA: (requirements configurable by developer)
            - textQuery: REQUIRED - natural language query
            - graphHints: OPTIONAL - use when entity/node mentioned in query
              - entityLabel: REQUIRED if graphHints provided (from schema node types above)
              - entityProperty: REQUIRED if graphHints provided (from schema properties above)
              - entityValue: REQUIRED if graphHints provided (actual value to match)
              - traverseRelations: OPTIONAL (array from schema relations above)
              - depth: OPTIONAL (integer 1-5, default: 2)
            - semanticKeywords: OPTIONAL (boost semantic search)
            
            EXAMPLES:
            1. Pure semantic: {"textQuery": "what are diabetes symptoms?"}
            2. Entity-aware: {"textQuery": "what conditions does John have?", "graphHints": {"entityLabel": "Patient", "entityProperty": "name", "entityValue": "John"}}
            3. Full traversal: {"textQuery": "medications for patient John", "graphHints": {"entityLabel": "Patient", "entityProperty": "name", "entityValue": "John", "traverseRelations": ["HAS_CONDITION", "TREATED_WITH"], "depth": 3}}
            """.formatted(schemaDescription);
    }
    
    private String formatSchemaForLLM(GraphSchema schema) {
        StringBuilder sb = new StringBuilder();
        sb.append("Node Types: ");
        schema.getNodeTypes().forEach(nt -> 
            sb.append(String.format("%s(%s), ", 
                nt.getLabel(), 
                String.join(",", nt.getPropertyNames())
            ))
        );
        sb.append("\nRelation Types: ");
        schema.getRelationTypes().forEach(rt ->
            sb.append(String.format("%s(%s→%s), ", 
                rt.getName(), 
                rt.getSourceLabel(), 
                rt.getTargetLabel()
            ))
        );
        return sb.toString();
    }
}
```

### Schema Registry

```java
@Component
public class GraphSchemaRegistry {
    private GraphSchema cachedSchema;
    
    @PostConstruct
    public void buildSchema() {
        // Auto-discover from database at startup
        List<NodeTypeSchema> nodeTypes = discoverNodeTypes();
        List<RelationTypeSchema> relationTypes = discoverRelationTypes();
        
        cachedSchema = new GraphSchema(nodeTypes, relationTypes);
    }
    
    private List<NodeTypeSchema> discoverNodeTypes() {
        return nodeRepository.findDistinctLabels()
            .flatMap(label -> {
                return nodeRepository.analyzePropertiesForLabel(label)
                    .map(props -> new NodeTypeSchema(
                        label,
                        props.keySet().stream().toList(),
                        nodeRepository.countByLabel(label).block()
                    ));
            })
            .collectList()
            .block();
    }
    
    private List<RelationTypeSchema> discoverRelationTypes() {
        return relationTypeRepository.findAll()
            .map(rt -> new RelationTypeSchema(
                rt.getName(),
                rt.getSemantics(),
                extractSourceLabel(rt),
                extractTargetLabel(rt)
            ))
            .collectList()
            .block();
    }
}

public record GraphSchema(
    List<NodeTypeSchema> nodeTypes,
    List<RelationTypeSchema> relationTypes
) {}

public record NodeTypeSchema(
    String label,
    List<String> propertyNames,
    long count
) {}

public record RelationTypeSchema(
    String name,
    RelationSemantics semantics,
    String sourceLabel,
    String targetLabel
) {}
```

---

## LLM Client System

### LLMClient - Base Interface

```java
/**
 * Base interface for LLM clients.
 * Framework provides this and common implementations.
 */
public interface LLMClient {
    
    /**
     * Generate text from prompt (blocking)
     */
    String generate(String prompt, LLMConfig config);
    
    /**
     * Generate with streaming (reactive)
     */
    Flux<String> generateStream(String prompt, LLMConfig config);
    
    /**
     * Chat completion with messages
     */
    ChatResponse chat(List<ChatMessage> messages, LLMConfig config);
    
    /**
     * Chat with streaming
     */
    Flux<ChatResponse> chatStream(List<ChatMessage> messages, LLMConfig config);
    
    /**
     * Check if client is available/healthy
     */
    boolean isAvailable();
    
    /**
     * Get model name/identifier
     */
    String getModelName();
    
    /**
     * Get provider name (openai, ollama, vllm, etc.)
     */
    String getProvider();
}
```

### LLMClientFactory - Factory for LLM Clients

```java
/**
 * Factory for creating LLM clients based on configuration.
 * Framework provides this with built-in implementations.
 */
@Component
public class LLMClientFactory {
    
    private final Map<String, LLMClient> clients = new ConcurrentHashMap<>();
    
    /**
     * Create or get cached LLM client
     */
    public LLMClient getClient(String provider, LLMConfig config) {
        String cacheKey = provider + ":" + config.getModel();
        
        return clients.computeIfAbsent(cacheKey, k -> createClient(provider, config));
    }
    
    /**
     * Create client from configuration
     */
    public LLMClient createClient(String provider, LLMConfig config) {
        return switch (provider.toLowerCase()) {
            case "openai" -> new OpenAILLMClient(config);
            case "ollama" -> new OllamaLLMClient(config);
            case "vllm" -> new VLLMLLMClient(config);
            case "mock" -> new MockLLMClient(config);
            default -> throw new IllegalArgumentException("Unknown LLM provider: " + provider);
        };
    }
    
    /**
     * Register custom client
     */
    public void registerClient(String name, LLMClient client) {
        clients.put(name, client);
    }
}
```

### Built-in LLM Clients

```java
/**
 * OpenAI client implementation
 */
public class OpenAILLMClient implements LLMClient {
    
    private final OpenAiService service;
    private final String model;
    
    public OpenAILLMClient(LLMConfig config) {
        this.service = new OpenAiService(config.getApiKey(), config.getTimeout());
        this.model = config.getModel();
    }
    
    @Override
    public String generate(String prompt, LLMConfig config) {
        CompletionRequest request = CompletionRequest.builder()
            .model(model)
            .prompt(prompt)
            .temperature(config.getTemperature())
            .maxTokens(config.getMaxTokens())
            .build();
        
        return service.createCompletion(request)
            .getChoices().get(0).getText();
    }
    
    @Override
    public ChatResponse chat(List<ChatMessage> messages, LLMConfig config) {
        ChatCompletionRequest request = ChatCompletionRequest.builder()
            .model(model)
            .messages(messages)
            .temperature(config.getTemperature())
            .maxTokens(config.getMaxTokens())
            .build();
        
        ChatCompletionResult result = service.createChatCompletion(request);
        return ChatResponse.from(result);
    }
    
    // ... streaming implementations
}

/**
 * Ollama client implementation (local models)
 */
public class OllamaLLMClient implements LLMClient {
    
    private final OkHttpClient httpClient;
    private final String baseUrl;
    private final String model;
    
    public OllamaLLMClient(LLMConfig config) {
        this.httpClient = new OkHttpClient.Builder()
            .connectTimeout(config.getTimeout())
            .build();
        this.baseUrl = config.getBaseUrl();
        this.model = config.getModel();
    }
    
    @Override
    public String generate(String prompt, LLMConfig config) {
        // HTTP request to Ollama API
        // ... implementation
    }
    
    // ... other methods
}

/**
 * vLLM client implementation (self-hosted)
 */
public class VLLMLLMClient implements LLMClient {
    // Similar to OpenAI API (OpenAI-compatible)
}

/**
 * Mock client for testing
 */
public class MockLLMClient implements LLMClient {
    
    @Override
    public String generate(String prompt, LLMConfig config) {
        return "Mock response for: " + prompt.substring(0, Math.min(50, prompt.length()));
    }
    
    @Override
    public ChatResponse chat(List<ChatMessage> messages, LLMConfig config) {
        return new ChatResponse("Mock chat response", "mock-model", 0);
    }
    
    @Override
    public boolean isAvailable() {
        return true;
    }
}
```

---

## DefaultContextProviderTool

### ContextProviderTool - Multi-Query Context Assembly

```java
/**
 * Default context provider tool that:
 * 1. Accepts multiple queries
 * 2. Retrieves context for each query
 * 3. Assembles results within token budget
 * 4. Deduplicates and ranks chunks
 * 
 * Framework provides this as the main tool for LLM integration.
 */
@Component
@MCPTool(
    name = "provide_context",
    description = "Retrieve and assemble context for multiple queries within token budget"
)
public class DefaultContextProviderTool {
    
    private final KnowledgeRetriever retriever;
    private final TokenCounter tokenCounter;
    
    /**
     * Provide context for multiple queries with token budget management
     * 
     * @param request Context request with queries and budget
     * @return Assembled context ready for LLM prompt
     */
    public ContextResponse provideContext(ContextRequest request) {
               long startTime = System.currentTimeMillis();
        
        // Step 1: Retrieve context for each query
        List<SearchResult> allResults = new ArrayList<>();
        for (QueryInput query : request.getQueries()) {
            SearchResult result = retriever.retrieve(
                RetrievalRequest.builder()
                    .startNodeId(query.getStartNodeId())
                    .query(query.getQuery())
                    .depth(query.getDepth() != null ? query.getDepth() : request.getDefaultDepth())
                    .topK(request.getTopKPerQuery())
                    .build()
            );
            allResults.add(result);
        }
        
        // Step 2: Deduplicate chunks across queries
        Map<UUID, RankedChunk> uniqueChunks = deduplicateChunks(allResults);
        
        // Step 3: Rank chunks globally
        List<RankedChunk> rankedChunks = rankChunks(uniqueChunks.values(), request);
        
        // Step 4: Assemble within token budget
        AssembledContext assembled = assembleWithinBudget(
            rankedChunks,
                       allResults,
            request.getTokenBudget()
        );
        
        long duration = System.currentTimeMillis() - startTime;
        
        return new ContextResponse(
            assembled.getChunks(),
            assembled.getReasoningContexts(),
            assembled.getTotalTokens(),
            request.getTokenBudget(),
            duration,
            allResults.stream()
                .map(SearchResult::strategy)
                .distinct()
                .toList(),
            buildMetadata(allResults, assembled)
        );
    }
    
    /**
     * Deduplicate chunks that appear in multiple search results
     * Keep highest score for each chunk
     */
    private Map<UUID, RankedChunk> deduplicateChunks(List<SearchResult> results) {
        Map<UUID, RankedChunk> uniqueChunks = new HashMap<>();
        
        for (SearchResult result : results) {
            for (RankedChunk chunk : result.results()) {
                UUID chunkId = chunk.chunk().getId();
                
                // Keep chunk with highest score
                uniqueChunks.merge(
                    chunkId,
                    chunk,
                    (existing, newChunk) -> 
                        existing.score() > newChunk.score() ? existing : newChunk
                );
            }
        }
        
        return uniqueChunks;
    }
    
    /**
     * Rank chunks globally using configured ranking strategy
     */
    private List<RankedChunk> rankChunks(
        Collection<RankedChunk> chunks,
        ContextRequest request
    ) {
        return chunks.stream()
            .sorted(Comparator.comparingDouble(RankedChunk::score).reversed())
            .toList();
    }
    
    /**
     * Assemble chunks within token budget
     * Priority: higher-ranked chunks first
     */
    private AssembledContext assembleWithinBudget(
        List<RankedChunk> rankedChunks,
        List<SearchResult> searchResults,
        int tokenBudget
    ) {
        List<RankedChunk> selected = new ArrayList<>();
        List<ReasoningContext> reasoningContexts = new ArrayList<>();
        int totalTokens = 0;
        
        // Reserve tokens for reasoning contexts (graphs, paths)
        int reasoningTokens = calculateReasoningTokens(searchResults);
        int availableForChunks = tokenBudget - reasoningTokens;
        
        // Add chunks until budget exhausted
        for (RankedChunk chunk : rankedChunks) {
            int chunkTokens = tokenCounter.count(chunk.chunk().getContent());
            
            if (totalTokens + chunkTokens <= availableForChunks) {
                selected.add(chunk);
                totalTokens += chunkTokens;
            }
            
            if (totalTokens >= availableForChunks) {
                break;
            }
        }
        
        // Add reasoning contexts
        for (SearchResult result : searchResults) {
            if (result.reasoningContext() != null) {
                reasoningContexts.add(result.reasoningContext());
            }
        }
        
        totalTokens += reasoningTokens;
        
        return new AssembledContext(
            selected,
            reasoningContexts,
            totalTokens
        );
    }
    
    /**
     * Calculate tokens needed for reasoning contexts
     * (graph structure, paths, keywords)
     */
    private int calculateReasoningTokens(List<SearchResult> results) {
        int total = 0;
        
        for (SearchResult result : results) {
            if (result.reasoningContext() != null) {
                ReasoningContext ctx = result.reasoningContext();
                
                // Estimate tokens for paths and keywords
                total += ctx.paths().size() * 20;  // ~20 tokens per path
                total += ctx.contextKeywords().size() * 2;  // ~2 tokens per keyword
                total += 50;  // Overhead for structure
            }
        }
        
        return total;
    }
    
    private Map<String, Object> buildMetadata(
        List<SearchResult> results,
        AssembledContext assembled
    ) {
        return Map.of(
            "queries_processed", results.size(),
            "total_chunks_retrieved", results.stream()
                .mapToInt(r -> r.results().size())
                .sum(),
            "chunks_selected", assembled.getChunks().size(),
            "token_utilization", (double) assembled.getTotalTokens() / tokenCounter.count(assembled.toString()),
            "strategies_used", results.stream()
                .map(SearchResult::strategy)
                .distinct()
                .toList()
        );
    }
}
```

### ContextRequest - Input for Context Provider

```java
/**
 * Request for context assembly with multiple queries
 */
public record ContextRequest(
    List<QueryInput> queries,             // Multiple queries to process
    int tokenBudget,                      // Total token budget
    int topKPerQuery,                     // Top K results per query
    Integer defaultDepth,                 // Default graph depth
    RankingStrategy rankingStrategy,      // How to rank chunks globally
    boolean includeReasoningContext       // Include graph paths/keywords
) {
    public static Builder builder() {
        return new Builder();
    }
    
    public static class Builder {
        private List<QueryInput> queries = new ArrayList<>();
        private int tokenBudget = 4000;
        private int topKPerQuery = 10;
        private Integer defaultDepth = 2;
        private RankingStrategy rankingStrategy = RankingStrategy.SCORE_DESCENDING;
        private boolean includeReasoningContext = true;
        
        public Builder addQuery(String query) {
            this.queries.add(new QueryInput(query, null, null));
            return this;
        }
        
        public Builder addQuery(String query, UUID startNodeId, Integer depth) {
            this.queries.add(new QueryInput(query, startNodeId, depth));
            return this;
        }
        
        public Builder tokenBudget(int budget) {
            this.tokenBudget = budget;
            return this;
        }
        
        public Builder topKPerQuery(int topK) {
            this.topKPerQuery = topK;
            return this;
        }
        
        public ContextRequest build() {
            return new ContextRequest(
                queries, tokenBudget, topKPerQuery, 
                defaultDepth, rankingStrategy, includeReasoningContext
            );
        }
    }
}

/**
 * Single query input
 */
public record QueryInput(
    String query,                         // Natural language query
    UUID startNodeId,                     // Optional: starting node for reasoning
    Integer depth                         // Optional: override default depth
) {}
```

### ContextResponse - Output from Context Provider

```java
/**
 * Response with assembled context
 */
public record ContextResponse(
    List<RankedChunk> chunks,             // Selected chunks within budget
    List<ReasoningContext> reasoningContexts,  // Graph contexts
    int totalTokens,                      // Actual tokens used
    int tokenBudget,                      // Budget provided
    long durationMs,                      // Execution time
    List<SearchStrategy> strategiesUsed,  // Which strategies were used
    Map<String, Object> metadata          // Additional info
) {
    /**
     * Format as prompt-ready string
     */
    public String toPromptContext() {
        StringBuilder sb = new StringBuilder();
        
        // Add reasoning contexts (graph structure)
        if (!reasoningContexts.isEmpty()) {
            sb.append("## Knowledge Graph Context\n\n");
            for (ReasoningContext ctx : reasoningContexts) {
                sb.append(formatReasoningContext(ctx));
            }
            sb.append("\n");
        }
        
        // Add chunks (documents, facts)
        sb.append("## Retrieved Information\n\n");
        for (int i = 0; i < chunks.size(); i++) {
            RankedChunk chunk = chunks.get(i);
            sb.append(String.format("[%d] (Score: %.3f)\n", i + 1, chunk.score()));
            sb.append(chunk.chunk().getContent());
            sb.append("\n\n");
        }
        
        return sb.toString();
    }
    
    private String formatReasoningContext(ReasoningContext ctx) {
        StringBuilder sb = new StringBuilder();
        sb.append("Starting from: ").append(ctx.rootNode().getLabel()).append("\n");
        sb.append("Related entities: ").append(
            ctx.relatedNodes().stream()
                .map(Node::getLabel)
                .collect(Collectors.joining(", "))
        ).append("\n");
        sb.append("Key concepts: ").append(
            String.join(", ", ctx.contextKeywords())
        ).append("\n");
        return sb.toString();
    }
}
```

### TokenCounter - Utility for Token Counting

```java
/**
 * Utility for counting tokens.
 * Framework provides basic implementation.
 */
@Component
public class TokenCounter {
    
    private static final double AVG_CHARS_PER_TOKEN = 4.0;
    
    /**
     * Count tokens in text (approximation)
     * For production, use tiktoken or model-specific tokenizer
     */
    public int count(String text) {
        if (text == null || text.isEmpty()) {
            return 0;
        }
        
        // Simple approximation: 1 token ≈ 4 characters
        return (int) Math.ceil(text.length() / AVG_CHARS_PER_TOKEN);
    }
    
    /**
     * Count tokens for a list of strings
     */
    public int countAll(List<String> texts) {
        return texts.stream()
            .mapToInt(this::count)
            .sum();
    }
}
```

---

## Configuration

```yaml
cef:
  # Storage
  storage:
    type: postgresql
    connection:
      url: jdbc:postgresql://localhost:5432/cef
      username: cef_user
      password: ${DB_PASSWORD}
    schemas:
      graph: cef_graph
      vector: cef_vector
  
  # In-Memory Graph
  graph:
    preload-on-startup: true
    max-nodes: 100000
    max-edges: 500000
  
  # Embedding
  embedding:
    provider: ollama
    model: nomic-embed-text
    dimension: 768
    base-url: http://localhost:11434
  
  # LLM Client
  llm:
    provider: ollama
    model: llama3
    base-url: http://localhost:11434
    temperature: 0.7
    max-tokens: 1000
    timeout: 60s
  
  # Search
  search:
    default-depth: 2
    default-top-k: 10
    min-results-threshold: 3
  
  # Context Provider
  context:
    default-token-budget: 4000
    default-top-k-per-query: 10
    ranking-strategy: score_descending
  
  # Data Sources
  datasources:
    filesystem:
      enabled: true
      root-path: data/seed
    
    minio:
      enabled: false
      endpoint: http://localhost:9000
      access-key: ${MINIO_ACCESS_KEY}
      secret-key: ${MINIO_SECRET_KEY}
      bucket: cef-data
      prefix: seed/
  
  # Parsers (built-in, users can override)
  parsers:
    yaml:
      enabled: true
      order: 100
    csv:
      enabled: true
      order: 100
    json:
      enabled: true
      order: 100
```

---

## Dependencies

```xml
<!-- ANTLR for parsing -->
<dependency>
    <groupId>org.antlr</groupId>
    <artifactId>antlr4-runtime</artifactId>
    <version>4.13.1</version>
</dependency>

<!-- JGraphT for in-memory graph -->
<dependency>
    <groupId>org.jgrapht</groupId>
    <artifactId>jgrapht-core</artifactId>
    <version>1.5.2</version>
</dependency>

<!-- SnakeYAML for YAML parsing -->
<dependency>
    <groupId>org.yaml</groupId>
    <artifactId>snakeyaml</artifactId>
    <version>2.2</version>
</dependency>

<!-- OpenCSV for CSV parsing -->
<dependency>
    <groupId>com.opencsv</groupId>
    <artifactId>opencsv</artifactId>
    <version>5.9</version>
</dependency>

<!-- AWS SDK for S3 -->
<dependency>
    <groupId>software.amazon.awssdk</groupId>
    <artifactId>s3</artifactId>
    <version>2.20.0</version>
</dependency>

<!-- OkHttp for Ollama/vLLM clients -->
<dependency>
    <groupId>com.squareup.okhttp3</groupId>
    <artifactId>okhttp</artifactId>
    <version>4.12.0</version>
</dependency>

<!-- OpenAI Java Client -->
<dependency>
    <groupId>com.theokanning.openai-gpt3-java</groupId>
    <artifactId>service</artifactId>
    <version>0.18.2</version>
</dependency>
```

---

## Summary - Framework Extensions

### Parser System
✅ **AbstractParser** - Base class for domain parsers  
✅ **ParserFactory** - Registry with order-based selection  
✅ **Built-in parsers** - YAML, CSV, JSON (ANTLR-based)  
✅ **User parsers** - Override with higher priority (lower order)  

### DataSource System
✅ **DataSource** interface - Abstraction over storage  
✅ **FileSystemDataSource** - Local files  
✅ **BlobStorageDataSource** - S3/MinIO compatible  
✅ **ResourceInfo** - Metadata about resources  

### LLM Client System
✅ **LLMClient** interface - Base for all clients  
✅ **LLMClientFactory** - Provider-based creation  
✅ **Built-in clients** - OpenAI, Ollama, vLLM, Mock  
✅ **Streaming support** - Reactive (Flux)  

### Context Provider Tool
✅ **DefaultContextProviderTool** - Multi-query assembly  
✅ **Token budget management** - Stay within limits  
✅ **Deduplication** - Across multiple queries  
✅ **Ranking** - Global chunk scoring  
✅ **MCP tool** - Ready for LLM integration
