#!/usr/bin/env python3
"""Benchmark report evaluator.

This script parses the Markdown benchmark reports generated by the CEF framework
and produces aggregate statistics that can be used to validate consistency
between the Medical and SAP benchmark suites. It focuses on industry-standard
metrics such as latency deltas, percentage improvements, chunk parity, and raw
result coverage.
"""
from __future__ import annotations

import argparse
import json
import math
import re
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, Iterable, List, Optional

# Regular expressions reused across parsing steps. Compiled once for efficiency.
_REPORT_METADATA_RE = re.compile(r"\*\*(Domain|Date):\*\*\s*(.+)")
_SCENARIO_HEADER_RE = re.compile(r"##\s+(\d+)\.\s+([^\n]+)\n", re.MULTILINE)
_TABLE_BLOCK_RE = re.compile(
    r"\|\s*Metric\s*\|.*?\|(.*?)\n\n",
    re.DOTALL,
)
_RAW_BLOCK_RE = re.compile(
    r"\*\*(Vector-Only \(Naive RAG\)|Knowledge Model \(Graph RAG\)):\*\*\s*```(.*?)```",
    re.DOTALL,
)


@dataclass
class ScenarioMetrics:
    """Parsed metrics for a single scenario."""

    index: int
    name: str
    objective: str
    query: str
    vector_latency_ms: float
    knowledge_latency_ms: float
    reported_improvement_pct: float
    reported_direction: str
    vector_chunks: int
    knowledge_chunks: int
    raw_vector_count: int
    raw_knowledge_count: int

    @property
    def latency_delta_ms(self) -> float:
        return self.knowledge_latency_ms - self.vector_latency_ms

    @property
    def computed_improvement_pct(self) -> float:
        if self.vector_latency_ms == 0:
            return float("nan")
        return (self.latency_delta_ms / self.vector_latency_ms) * 100.0

    @property
    def improvement_gap_pct_points(self) -> float:
        return self.computed_improvement_pct - self.reported_improvement_pct

    @property
    def chunks_in_sync(self) -> bool:
        return (
            self.vector_chunks == self.knowledge_chunks
            == self.raw_vector_count
            == self.raw_knowledge_count
        )


@dataclass
class ReportEvaluation:
    """Domain-level evaluation produced from a benchmark report."""

    path: Path
    title: str
    domain: str
    generated_at: str
    scenarios: List[ScenarioMetrics]

    @property
    def averages(self) -> Dict[str, float]:
        vector_latencies = [s.vector_latency_ms for s in self.scenarios]
        knowledge_latencies = [s.knowledge_latency_ms for s in self.scenarios]
        reported = [s.reported_improvement_pct for s in self.scenarios]
        computed = [s.computed_improvement_pct for s in self.scenarios]
        return {
            "vector_latency_ms": _mean(vector_latencies),
            "knowledge_latency_ms": _mean(knowledge_latencies),
            "reported_improvement_pct": _mean(reported),
            "computed_improvement_pct": _mean(computed),
        }

    @property
    def chunk_parity_ratio(self) -> float:
        if not self.scenarios:
            return 1.0
        matches = sum(1 for s in self.scenarios if s.chunks_in_sync)
        return matches / len(self.scenarios)

    @property
    def knowledge_faster_ratio(self) -> float:
        if not self.scenarios:
            return 0.0
        faster = sum(1 for s in self.scenarios if s.latency_delta_ms < 0)
        return faster / len(self.scenarios)


def _mean(values: Iterable[float]) -> float:
    numeric = [v for v in values if not math.isnan(v)]
    if not numeric:
        return float("nan")
    return sum(numeric) / len(numeric)


def parse_report(path: Path) -> ReportEvaluation:
    text = path.read_text(encoding="utf-8")
    title = text.splitlines()[0].lstrip("# ").strip()

    metadata: Dict[str, str] = {}
    header_block = text.split("---", 1)[0]
    for match in _REPORT_METADATA_RE.finditer(header_block):
        key, value = match.groups()
        metadata[key.lower()] = value.strip()

    domain = metadata.get("domain", "Unknown")
    generated_at = metadata.get("date", "Unknown")

    scenarios: List[ScenarioMetrics] = []

    # Iterate through scenario sections by locating headers and slicing text.
    header_matches = list(_SCENARIO_HEADER_RE.finditer(text))
    for idx, header in enumerate(header_matches):
        scenario_index = int(header.group(1))
        scenario_name = header.group(2).strip()
        start = header.end()
        end = header_matches[idx + 1].start() if idx + 1 < len(header_matches) else len(text)
        body = text[start:end]

        objective_match = re.search(r"\*\*Objective:\*\*\s*(.+)", body)
        query_match = re.search(r'\*\*Query:\*\*\s*"(.+?)"', body)
        objective = objective_match.group(1).strip() if objective_match else ""
        query = query_match.group(1).strip() if query_match else ""

        table_match = _TABLE_BLOCK_RE.search(body)
        if not table_match:
            raise ValueError(f"Failed to locate metric table in scenario '{scenario_name}'")
        table_text = table_match.group(1)
        metrics = _parse_metrics_table(table_text)

        raw_counts = _parse_raw_blocks(body)

        if "Chunks Retrieved" not in metrics:
            raise ValueError(
                f"Missing 'Chunks Retrieved' row in scenario '{scenario_name}'. "
                f"Available rows: {list(metrics.keys())}"
            )

        scenario = ScenarioMetrics(
            index=scenario_index,
            name=scenario_name,
            objective=objective,
            query=query,
            vector_latency_ms=metrics["Latency"]["vector"],
            knowledge_latency_ms=metrics["Latency"]["knowledge"],
            reported_improvement_pct=metrics["Latency"]["improvement_pct"],
            reported_direction=metrics["Latency"]["direction"],
            vector_chunks=int(metrics["Chunks Retrieved"]["vector"]),
            knowledge_chunks=int(metrics["Chunks Retrieved"]["knowledge"]),
            raw_vector_count=raw_counts.get("Vector-Only (Naive RAG)", 0),
            raw_knowledge_count=raw_counts.get("Knowledge Model (Graph RAG)", 0),
        )
        scenarios.append(scenario)

    return ReportEvaluation(
        path=path,
        title=title,
        domain=domain,
        generated_at=generated_at,
        scenarios=scenarios,
    )


def _parse_metrics_table(table_text: str) -> Dict[str, Dict[str, float]]:
    metrics: Dict[str, Dict[str, float]] = {}
    for line in table_text.strip().splitlines():
        if not line.strip().startswith("|"):
            continue
        columns = [col.strip() for col in line.strip().strip("|").split("|")]
        if len(columns) != 4:
            continue
        if all(
            not col or set(col.replace(" ", "")) <= set("-:") for col in columns
        ):
            continue
        label = " ".join(_strip_markup(columns[0]).split())
        if label.lower() == "metric":
            continue
        vector_value = _parse_numeric(columns[1])
        knowledge_value = _parse_numeric(columns[2])
        improvement_value, direction = _parse_improvement(columns[3])
        metrics[label] = {
            "vector": vector_value,
            "knowledge": knowledge_value,
            "improvement_pct": improvement_value,
            "direction": direction,
        }
    if not metrics:
        raise ValueError("Metrics table parsing yielded no rows")
    return metrics


def _strip_markup(value: str) -> str:
    return value.replace("*", "").strip()


def _parse_numeric(raw: str) -> float:
    cleaned = raw.replace("ms", "").strip()
    try:
        return float(cleaned)
    except ValueError:
        raise ValueError(f"Unable to parse numeric value from '{raw}'")


def _parse_improvement(raw: str) -> (float, str):
    cleaned = raw.strip().replace("%", "")
    direction = ""
    if cleaned and not cleaned[0].isdigit() and cleaned[0] not in {"+", "-"}:
        direction = cleaned[0]
        cleaned = cleaned[1:].strip()
    if cleaned in {"", "-", "–", "—"}:
        return 0.0, direction
    try:
        value = float(cleaned)
    except ValueError as exc:
        raise ValueError(f"Unable to parse improvement value from '{raw}'") from exc
    if direction == "↓":
        value = -abs(value)
    elif direction == "↑":
        value = abs(value)
    return value, direction


def _parse_raw_blocks(body: str) -> Dict[str, int]:
    counts: Dict[str, int] = {}
    for label, contents in _RAW_BLOCK_RE.findall(body):
        rows = [line for line in contents.strip().splitlines() if line.strip()]
        counts[label] = len(rows)
    return counts


def evaluate_reports(paths: List[Path]) -> List[ReportEvaluation]:
    evaluations: List[ReportEvaluation] = []
    for path in paths:
        if not path.exists():
            raise FileNotFoundError(f"Report not found: {path}")
        evaluations.append(parse_report(path))
    return evaluations


def _format_percentage(value: float) -> str:
    if math.isnan(value):
        return "nan"
    sign = "+" if value >= 0 else ""
    return f"{sign}{value:.1f}%"


def print_summary(evaluations: List[ReportEvaluation]) -> None:
    if not evaluations:
        print("No reports evaluated.")
        return

    print("=" * 96)
    print("Benchmark Evaluation Summary")
    print("=" * 96)

    total_scenarios = sum(len(report.scenarios) for report in evaluations)

    for report in evaluations:
        averages = report.averages
        print(f"Report: {report.title}")
        print(f"  Path: {report.path}")
        print(f"  Domain: {report.domain}")
        print(f"  Generated At: {report.generated_at}")
        print(f"  Scenarios: {len(report.scenarios)}")
        print(f"  Average Latency (Vector -> KM): {averages['vector_latency_ms']:.1f}ms -> {averages['knowledge_latency_ms']:.1f}ms")
        print(
            "  Average Improvement (reported -> computed): "
            f"{_format_percentage(averages['reported_improvement_pct'])} -> {_format_percentage(averages['computed_improvement_pct'])}"
        )
        print(
            f"  Chunk Parity: {report.chunk_parity_ratio * 100:.1f}% scenarios align | "
            f"Knowledge Faster: {report.knowledge_faster_ratio * 100:.1f}%"
        )
        print("  Scenario Checks:")

        for scenario in report.scenarios:
            print(f"    {scenario.index}. {scenario.name}")
            print(f"       Query: {scenario.query}")
            print(
                f"       Latency: {scenario.vector_latency_ms:.1f}ms -> {scenario.knowledge_latency_ms:.1f}ms "
                f"(Δ {scenario.latency_delta_ms:+.1f}ms | reported {_format_percentage(scenario.reported_improvement_pct)} | "
                f"computed {_format_percentage(scenario.computed_improvement_pct)} | gap {scenario.improvement_gap_pct_points:+.1f}pp)"
            )
            print(
                f"       Chunks: V={scenario.vector_chunks} / KM={scenario.knowledge_chunks} | Raw rows: V={scenario.raw_vector_count}, KM={scenario.raw_knowledge_count} | "
                f"Parity: {'yes' if scenario.chunks_in_sync else 'no'}"
            )
        print("-" * 96)

    overall_vector = _mean([s.vector_latency_ms for r in evaluations for s in r.scenarios])
    overall_knowledge = _mean([s.knowledge_latency_ms for r in evaluations for s in r.scenarios])
    overall_reported = _mean([s.reported_improvement_pct for r in evaluations for s in r.scenarios])
    overall_computed = _mean([s.computed_improvement_pct for r in evaluations for s in r.scenarios])
    chunk_parity = sum(s.chunks_in_sync for r in evaluations for s in r.scenarios)
    knowledge_faster = sum(s.latency_delta_ms < 0 for r in evaluations for s in r.scenarios)

    print("Overall Averages Across Reports:")
    print(f"  Scenarios Evaluated: {total_scenarios}")
    print(f"  Latency (Vector -> KM): {overall_vector:.1f}ms -> {overall_knowledge:.1f}ms")
    print(
        "  Improvement (reported -> computed): "
        f"{_format_percentage(overall_reported)} -> {_format_percentage(overall_computed)}"
    )
    print(
        f"  Chunk Parity: {(chunk_parity / total_scenarios) * 100:.1f}% | "
        f"Knowledge Faster: {(knowledge_faster / total_scenarios) * 100:.1f}%"
    )
    print("=" * 96)


def to_json(evaluations: List[ReportEvaluation]) -> str:
    payload = []
    for report in evaluations:
        scenarios = [asdict(scenario) for scenario in report.scenarios]
        payload.append(
            {
                "path": str(report.path),
                "title": report.title,
                "domain": report.domain,
                "generated_at": report.generated_at,
                "averages": report.averages,
                "chunk_parity_ratio": report.chunk_parity_ratio,
                "knowledge_faster_ratio": report.knowledge_faster_ratio,
                "scenarios": scenarios,
            }
        )
    return json.dumps(payload, indent=2)


def build_markdown(evaluations: List[ReportEvaluation]) -> str:
    timestamp = datetime.utcnow().isoformat() + "Z"
    total_reports = len(evaluations)
    total_scenarios = sum(len(r.scenarios) for r in evaluations)

    overall_vector = _mean([s.vector_latency_ms for r in evaluations for s in r.scenarios])
    overall_knowledge = _mean([s.knowledge_latency_ms for r in evaluations for s in r.scenarios])
    overall_reported = _mean([s.reported_improvement_pct for r in evaluations for s in r.scenarios])
    overall_computed = _mean([s.computed_improvement_pct for r in evaluations for s in r.scenarios])
    chunk_parity = sum(s.chunks_in_sync for r in evaluations for s in r.scenarios)
    knowledge_faster = sum(s.latency_delta_ms < 0 for r in evaluations for s in r.scenarios)

    lines: List[str] = []
    lines.append("# Benchmark Evaluation Summary")
    lines.append("")
    lines.append(f"Generated: {timestamp}")
    lines.append(f"Reports evaluated: {total_reports}")
    lines.append(f"Total scenarios: {total_scenarios}")
    lines.append("")

    for report in evaluations:
        averages = report.averages
        lines.append(f"## {report.title}")
        lines.append("")
        lines.append(f"- Path: `{report.path}`")
        lines.append(f"- Domain: {report.domain}")
        lines.append(f"- Generated at: {report.generated_at}")
        lines.append(
            f"- Average latency (vector -> knowledge): {averages['vector_latency_ms']:.1f}ms -> {averages['knowledge_latency_ms']:.1f}ms"
        )
        lines.append(
            "- Average improvement (reported -> computed): "
            f"{_format_percentage(averages['reported_improvement_pct'])} -> {_format_percentage(averages['computed_improvement_pct'])}"
        )
        lines.append(
            f"- Chunk parity: {report.chunk_parity_ratio * 100:.1f}% | Knowledge faster: {report.knowledge_faster_ratio * 100:.1f}%"
        )
        lines.append("")
        lines.append("### Scenarios")
        lines.append("")
        lines.append(
            "| # | Scenario | Query | Latency (ms) | Delta (ms) | Reported | Computed | Gap (pp) | Chunks (V/KM) | Raw Rows (V/KM) | Parity |"
        )
        lines.append(
            "|---|---------|-------|--------------|-----------|----------|----------|----------|----------------|------------------|--------|"
        )
        for scenario in report.scenarios:
            lines.append(
                "| {idx} | {name} | {query} | {latency} | {delta:+.1f} | {reported} | {computed} | {gap:+.1f} | {chunks} | {raws} | {parity} |".format(
                    idx=scenario.index,
                    name=_escape_pipes(scenario.name),
                    query=_escape_pipes(scenario.query),
                    latency=f"{scenario.vector_latency_ms:.1f} -> {scenario.knowledge_latency_ms:.1f}",
                    delta=scenario.latency_delta_ms,
                    reported=_format_percentage(scenario.reported_improvement_pct),
                    computed=_format_percentage(scenario.computed_improvement_pct),
                    gap=scenario.improvement_gap_pct_points,
                    chunks=f"{scenario.vector_chunks}/{scenario.knowledge_chunks}",
                    raws=f"{scenario.raw_vector_count}/{scenario.raw_knowledge_count}",
                    parity="yes" if scenario.chunks_in_sync else "no",
                )
            )
        lines.append("")

    lines.append("## Overall Summary")
    lines.append("")
    lines.append(f"- Scenarios evaluated: {total_scenarios}")
    lines.append(
        f"- Latency (vector -> knowledge): {overall_vector:.1f}ms -> {overall_knowledge:.1f}ms"
    )
    lines.append(
        "- Improvement (reported -> computed): "
        f"{_format_percentage(overall_reported)} -> {_format_percentage(overall_computed)}"
    )
    lines.append(
        f"- Chunk parity: {(chunk_parity / total_scenarios) * 100:.1f}% | Knowledge faster: {(knowledge_faster / total_scenarios) * 100:.1f}%"
    )

    return "\n".join(lines) + "\n"


def _escape_pipes(value: str) -> str:
    return value.replace("|", "\\|")


def main(argv: Optional[Iterable[str]] = None) -> None:
    module_root = Path(__file__).resolve().parents[4]
    default_reports = [
        module_root / "BENCHMARK_REPORT.md",
        module_root / "BENCHMARK_REPORT_2.md",
        module_root / "SAP_BENCHMARK_REPORT.md",
    ]

    parser = argparse.ArgumentParser(
        description="Evaluate benchmark reports for consistency and aggregate metrics.",
    )
    parser.add_argument(
        "--reports",
        nargs="*",
        type=Path,
        default=default_reports,
        help="Paths to Markdown reports. Defaults to the three generated benchmarks.",
    )
    parser.add_argument(
        "--format",
        choices=["text", "json"],
        default="text",
        help="Output format (text summary or JSON).",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=module_root / "eval-report.md",
        help="Path to save the Markdown evaluation report (default: eval-report.md).",
    )
    args = parser.parse_args(list(argv) if argv is not None else None)

    evaluations = evaluate_reports([path.expanduser() for path in args.reports])
    if args.format == "json":
        print(to_json(evaluations))
    else:
        print_summary(evaluations)

    markdown = build_markdown(evaluations)
    output_path = args.output.expanduser()
    output_path.write_text(markdown, encoding="utf-8")
    print(f"Markdown evaluation report written to {output_path}")


if __name__ == "__main__":
    main()
